{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A hello world program for classifying apples and oranges\n",
    "# This is based off youtube video https://www.youtube.com/watch?v=cKxRvEZd3Mw\n",
    "import sklearn\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in the second column, 1=smooth, and 0=bumpy\n",
    "features=[[140,1],[130,1],[150,0],[170,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in the labels column, 0=apple, and 1=orange\n",
    "labels=[0,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create empty classifier\n",
    "clf=tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now use fit function to find patterns in data and train the clasifier\n",
    "clf.fit(features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardorinaldi/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# now let's test the classifier on a new example\n",
    "# look at your original training set - this should certainly be an orange!\n",
    "print(clf.predict([145,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing a decision tree\n",
    "# Based on video https://www.youtube.com/watch?v=tNa99PG8hR8\n",
    "# Will use a dataset that has four features to describe three different types of irises\n",
    "# There are 150 examples total\n",
    "# Scikit already makes it easy to import iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the dataset equal to variable iris\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# let's do a few things to get comfortable working with the dataset before we do the classifier\n",
    "# you can see feature details\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# or you can see names of the flowers (labels) using \"target_names\"\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.1  3.5  1.4  0.2]\n"
     ]
    }
   ],
   "source": [
    "# since its an array, you can see specific examples (rows in the array using \"data\")\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# likewise using \"target\" gets you the labels for a particular row\n",
    "# 0 means it's a setosa\n",
    "print (iris.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0: label 0, features [ 5.1  3.5  1.4  0.2]\n",
      "Example 1: label 0, features [ 4.9  3.   1.4  0.2]\n",
      "Example 2: label 0, features [ 4.7  3.2  1.3  0.2]\n",
      "Example 3: label 0, features [ 4.6  3.1  1.5  0.2]\n",
      "Example 4: label 0, features [ 5.   3.6  1.4  0.2]\n",
      "Example 5: label 0, features [ 5.4  3.9  1.7  0.4]\n",
      "Example 6: label 0, features [ 4.6  3.4  1.4  0.3]\n",
      "Example 7: label 0, features [ 5.   3.4  1.5  0.2]\n",
      "Example 8: label 0, features [ 4.4  2.9  1.4  0.2]\n",
      "Example 9: label 0, features [ 4.9  3.1  1.5  0.1]\n",
      "Example 10: label 0, features [ 5.4  3.7  1.5  0.2]\n",
      "Example 11: label 0, features [ 4.8  3.4  1.6  0.2]\n",
      "Example 12: label 0, features [ 4.8  3.   1.4  0.1]\n",
      "Example 13: label 0, features [ 4.3  3.   1.1  0.1]\n",
      "Example 14: label 0, features [ 5.8  4.   1.2  0.2]\n",
      "Example 15: label 0, features [ 5.7  4.4  1.5  0.4]\n",
      "Example 16: label 0, features [ 5.4  3.9  1.3  0.4]\n",
      "Example 17: label 0, features [ 5.1  3.5  1.4  0.3]\n",
      "Example 18: label 0, features [ 5.7  3.8  1.7  0.3]\n",
      "Example 19: label 0, features [ 5.1  3.8  1.5  0.3]\n",
      "Example 20: label 0, features [ 5.4  3.4  1.7  0.2]\n",
      "Example 21: label 0, features [ 5.1  3.7  1.5  0.4]\n",
      "Example 22: label 0, features [ 4.6  3.6  1.   0.2]\n",
      "Example 23: label 0, features [ 5.1  3.3  1.7  0.5]\n",
      "Example 24: label 0, features [ 4.8  3.4  1.9  0.2]\n",
      "Example 25: label 0, features [ 5.   3.   1.6  0.2]\n",
      "Example 26: label 0, features [ 5.   3.4  1.6  0.4]\n",
      "Example 27: label 0, features [ 5.2  3.5  1.5  0.2]\n",
      "Example 28: label 0, features [ 5.2  3.4  1.4  0.2]\n",
      "Example 29: label 0, features [ 4.7  3.2  1.6  0.2]\n",
      "Example 30: label 0, features [ 4.8  3.1  1.6  0.2]\n",
      "Example 31: label 0, features [ 5.4  3.4  1.5  0.4]\n",
      "Example 32: label 0, features [ 5.2  4.1  1.5  0.1]\n",
      "Example 33: label 0, features [ 5.5  4.2  1.4  0.2]\n",
      "Example 34: label 0, features [ 4.9  3.1  1.5  0.1]\n",
      "Example 35: label 0, features [ 5.   3.2  1.2  0.2]\n",
      "Example 36: label 0, features [ 5.5  3.5  1.3  0.2]\n",
      "Example 37: label 0, features [ 4.9  3.1  1.5  0.1]\n",
      "Example 38: label 0, features [ 4.4  3.   1.3  0.2]\n",
      "Example 39: label 0, features [ 5.1  3.4  1.5  0.2]\n",
      "Example 40: label 0, features [ 5.   3.5  1.3  0.3]\n",
      "Example 41: label 0, features [ 4.5  2.3  1.3  0.3]\n",
      "Example 42: label 0, features [ 4.4  3.2  1.3  0.2]\n",
      "Example 43: label 0, features [ 5.   3.5  1.6  0.6]\n",
      "Example 44: label 0, features [ 5.1  3.8  1.9  0.4]\n",
      "Example 45: label 0, features [ 4.8  3.   1.4  0.3]\n",
      "Example 46: label 0, features [ 5.1  3.8  1.6  0.2]\n",
      "Example 47: label 0, features [ 4.6  3.2  1.4  0.2]\n",
      "Example 48: label 0, features [ 5.3  3.7  1.5  0.2]\n",
      "Example 49: label 0, features [ 5.   3.3  1.4  0.2]\n",
      "Example 50: label 1, features [ 7.   3.2  4.7  1.4]\n",
      "Example 51: label 1, features [ 6.4  3.2  4.5  1.5]\n",
      "Example 52: label 1, features [ 6.9  3.1  4.9  1.5]\n",
      "Example 53: label 1, features [ 5.5  2.3  4.   1.3]\n",
      "Example 54: label 1, features [ 6.5  2.8  4.6  1.5]\n",
      "Example 55: label 1, features [ 5.7  2.8  4.5  1.3]\n",
      "Example 56: label 1, features [ 6.3  3.3  4.7  1.6]\n",
      "Example 57: label 1, features [ 4.9  2.4  3.3  1. ]\n",
      "Example 58: label 1, features [ 6.6  2.9  4.6  1.3]\n",
      "Example 59: label 1, features [ 5.2  2.7  3.9  1.4]\n",
      "Example 60: label 1, features [ 5.   2.   3.5  1. ]\n",
      "Example 61: label 1, features [ 5.9  3.   4.2  1.5]\n",
      "Example 62: label 1, features [ 6.   2.2  4.   1. ]\n",
      "Example 63: label 1, features [ 6.1  2.9  4.7  1.4]\n",
      "Example 64: label 1, features [ 5.6  2.9  3.6  1.3]\n",
      "Example 65: label 1, features [ 6.7  3.1  4.4  1.4]\n",
      "Example 66: label 1, features [ 5.6  3.   4.5  1.5]\n",
      "Example 67: label 1, features [ 5.8  2.7  4.1  1. ]\n",
      "Example 68: label 1, features [ 6.2  2.2  4.5  1.5]\n",
      "Example 69: label 1, features [ 5.6  2.5  3.9  1.1]\n",
      "Example 70: label 1, features [ 5.9  3.2  4.8  1.8]\n",
      "Example 71: label 1, features [ 6.1  2.8  4.   1.3]\n",
      "Example 72: label 1, features [ 6.3  2.5  4.9  1.5]\n",
      "Example 73: label 1, features [ 6.1  2.8  4.7  1.2]\n",
      "Example 74: label 1, features [ 6.4  2.9  4.3  1.3]\n",
      "Example 75: label 1, features [ 6.6  3.   4.4  1.4]\n",
      "Example 76: label 1, features [ 6.8  2.8  4.8  1.4]\n",
      "Example 77: label 1, features [ 6.7  3.   5.   1.7]\n",
      "Example 78: label 1, features [ 6.   2.9  4.5  1.5]\n",
      "Example 79: label 1, features [ 5.7  2.6  3.5  1. ]\n",
      "Example 80: label 1, features [ 5.5  2.4  3.8  1.1]\n",
      "Example 81: label 1, features [ 5.5  2.4  3.7  1. ]\n",
      "Example 82: label 1, features [ 5.8  2.7  3.9  1.2]\n",
      "Example 83: label 1, features [ 6.   2.7  5.1  1.6]\n",
      "Example 84: label 1, features [ 5.4  3.   4.5  1.5]\n",
      "Example 85: label 1, features [ 6.   3.4  4.5  1.6]\n",
      "Example 86: label 1, features [ 6.7  3.1  4.7  1.5]\n",
      "Example 87: label 1, features [ 6.3  2.3  4.4  1.3]\n",
      "Example 88: label 1, features [ 5.6  3.   4.1  1.3]\n",
      "Example 89: label 1, features [ 5.5  2.5  4.   1.3]\n",
      "Example 90: label 1, features [ 5.5  2.6  4.4  1.2]\n",
      "Example 91: label 1, features [ 6.1  3.   4.6  1.4]\n",
      "Example 92: label 1, features [ 5.8  2.6  4.   1.2]\n",
      "Example 93: label 1, features [ 5.   2.3  3.3  1. ]\n",
      "Example 94: label 1, features [ 5.6  2.7  4.2  1.3]\n",
      "Example 95: label 1, features [ 5.7  3.   4.2  1.2]\n",
      "Example 96: label 1, features [ 5.7  2.9  4.2  1.3]\n",
      "Example 97: label 1, features [ 6.2  2.9  4.3  1.3]\n",
      "Example 98: label 1, features [ 5.1  2.5  3.   1.1]\n",
      "Example 99: label 1, features [ 5.7  2.8  4.1  1.3]\n",
      "Example 100: label 2, features [ 6.3  3.3  6.   2.5]\n",
      "Example 101: label 2, features [ 5.8  2.7  5.1  1.9]\n",
      "Example 102: label 2, features [ 7.1  3.   5.9  2.1]\n",
      "Example 103: label 2, features [ 6.3  2.9  5.6  1.8]\n",
      "Example 104: label 2, features [ 6.5  3.   5.8  2.2]\n",
      "Example 105: label 2, features [ 7.6  3.   6.6  2.1]\n",
      "Example 106: label 2, features [ 4.9  2.5  4.5  1.7]\n",
      "Example 107: label 2, features [ 7.3  2.9  6.3  1.8]\n",
      "Example 108: label 2, features [ 6.7  2.5  5.8  1.8]\n",
      "Example 109: label 2, features [ 7.2  3.6  6.1  2.5]\n",
      "Example 110: label 2, features [ 6.5  3.2  5.1  2. ]\n",
      "Example 111: label 2, features [ 6.4  2.7  5.3  1.9]\n",
      "Example 112: label 2, features [ 6.8  3.   5.5  2.1]\n",
      "Example 113: label 2, features [ 5.7  2.5  5.   2. ]\n",
      "Example 114: label 2, features [ 5.8  2.8  5.1  2.4]\n",
      "Example 115: label 2, features [ 6.4  3.2  5.3  2.3]\n",
      "Example 116: label 2, features [ 6.5  3.   5.5  1.8]\n",
      "Example 117: label 2, features [ 7.7  3.8  6.7  2.2]\n",
      "Example 118: label 2, features [ 7.7  2.6  6.9  2.3]\n",
      "Example 119: label 2, features [ 6.   2.2  5.   1.5]\n",
      "Example 120: label 2, features [ 6.9  3.2  5.7  2.3]\n",
      "Example 121: label 2, features [ 5.6  2.8  4.9  2. ]\n",
      "Example 122: label 2, features [ 7.7  2.8  6.7  2. ]\n",
      "Example 123: label 2, features [ 6.3  2.7  4.9  1.8]\n",
      "Example 124: label 2, features [ 6.7  3.3  5.7  2.1]\n",
      "Example 125: label 2, features [ 7.2  3.2  6.   1.8]\n",
      "Example 126: label 2, features [ 6.2  2.8  4.8  1.8]\n",
      "Example 127: label 2, features [ 6.1  3.   4.9  1.8]\n",
      "Example 128: label 2, features [ 6.4  2.8  5.6  2.1]\n",
      "Example 129: label 2, features [ 7.2  3.   5.8  1.6]\n",
      "Example 130: label 2, features [ 7.4  2.8  6.1  1.9]\n",
      "Example 131: label 2, features [ 7.9  3.8  6.4  2. ]\n",
      "Example 132: label 2, features [ 6.4  2.8  5.6  2.2]\n",
      "Example 133: label 2, features [ 6.3  2.8  5.1  1.5]\n",
      "Example 134: label 2, features [ 6.1  2.6  5.6  1.4]\n",
      "Example 135: label 2, features [ 7.7  3.   6.1  2.3]\n",
      "Example 136: label 2, features [ 6.3  3.4  5.6  2.4]\n",
      "Example 137: label 2, features [ 6.4  3.1  5.5  1.8]\n",
      "Example 138: label 2, features [ 6.   3.   4.8  1.8]\n",
      "Example 139: label 2, features [ 6.9  3.1  5.4  2.1]\n",
      "Example 140: label 2, features [ 6.7  3.1  5.6  2.4]\n",
      "Example 141: label 2, features [ 6.9  3.1  5.1  2.3]\n",
      "Example 142: label 2, features [ 5.8  2.7  5.1  1.9]\n",
      "Example 143: label 2, features [ 6.8  3.2  5.9  2.3]\n",
      "Example 144: label 2, features [ 6.7  3.3  5.7  2.5]\n",
      "Example 145: label 2, features [ 6.7  3.   5.2  2.3]\n",
      "Example 146: label 2, features [ 6.3  2.5  5.   1.9]\n",
      "Example 147: label 2, features [ 6.5  3.   5.2  2. ]\n",
      "Example 148: label 2, features [ 6.2  3.4  5.4  2.3]\n",
      "Example 149: label 2, features [ 5.9  3.   5.1  1.8]\n"
     ]
    }
   ],
   "source": [
    "# you can see the whole dataset this way\n",
    "for i in range(len(iris.target)):\n",
    "    print (\"Example %d: label %s, features %s\" % (i,iris.target[i],iris.data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's work on the classifier\n",
    "# we will need to set aside some example data to test the classifier's accuracy later\n",
    "# for this simple example, we'll remove just three rows - these happen to contain one of each type of iris\n",
    "# let's start at the beginning again, and add numpy functionality as well\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "iris=datasets.load_iris()\n",
    "# define the data rows you'll set aside for testing later\n",
    "test_idx=[0,50,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define training data as iris datset with testing rows deleted\n",
    "train_target=np.delete(iris.target, test_idx)\n",
    "train_data=np.delete(iris.data, test_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define testing data as just the rows in iris that you defined earlier\n",
    "test_target=iris.target[test_idx]\n",
    "test_data=iris.data[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's define a decision tree classifier and train it on our data like orange and apple example\n",
    "clf=tree.DecisionTreeClassifier()\n",
    "clf=clf.fit(train_data,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# now let's test the clssifier\n",
    "# look at the test target data\n",
    "print(test_target)\n",
    "# compare that to the classifier's predicted results\n",
    "print(clf.predict(test_data))\n",
    "# if they match you are looking good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new video https://www.youtube.com/watch?v=84gqSbLcBFE\n",
    "# we're going to use iris again\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this time we'll define data/features as x and labels/target as y\n",
    "# since the pattern is kind of like f(X)=y\n",
    "X=iris.data\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there is a handy function that makes it easier to partition training and test data\n",
    "# using .5 means half the data will be used for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's do decision tree again using the training data\n",
    "from sklearn import tree\n",
    "my_classifier=tree.DecisionTreeClassifier()\n",
    "my_classifier.fit(X_train,y_train)\n",
    "predictions=my_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 2 0 1 0 0 0 1 1 1 1 1 2 1 0 1 2 1 0 2 2 2 1 0 1 1 2 2 2 0 2 0 1 0 0\n",
      " 1 2 2 1 0 0 2 1 0 0 2 1 0 0 2 0 2 1 1 0 2 0 2 1 2 2 2 2 2 0 0 2 0 0 0 1 1\n",
      " 1]\n",
      "[1 0 2 2 0 1 0 0 0 1 1 1 1 1 2 1 0 1 2 1 0 2 2 2 1 0 1 1 2 2 2 0 2 0 2 0 0\n",
      " 1 2 2 1 0 0 2 1 0 0 2 1 0 0 2 0 1 1 1 0 2 0 2 1 2 2 2 2 2 0 0 1 0 0 0 1 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# compare predictions to the test labels\n",
    "print(y_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# too many to compare - how do we see how accurate it is?\n",
    "# use this accuracy function and put into it what you are comparing\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986666666667\n"
     ]
    }
   ],
   "source": [
    "# we can use an alternative approach instead of a decision tree\n",
    "# This is K Nearest neighbors\n",
    "# notice that while this is a different classifier, a lot of the syntax...\n",
    "# is the same, such as .fit and .predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier=KNeighborsClassifier()\n",
    "my_classifier.fit(X_train, y_train)\n",
    "predictions=my_classifier.predict(X_test)\n",
    "print (accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning recipes #5\n",
    "# Write your own classifier - a simple version of K nearest neighbors\n",
    "# Going to take our previous KNeighborsClassifier code but this time...\n",
    "# ...We'll write it ourselves\n",
    "\n",
    "# random is just temporarily used initially to populate the predictions before we write the real classifier code\n",
    "import random\n",
    "\n",
    "# So bringing back the code, but adding a class to it which we can call whatever we want\n",
    "class ScrappyKNN():\n",
    "# defining the fit method for the class - will require parameters from the training set as inputs\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train=X_train\n",
    "        self.y_train=y_train\n",
    "        pass\n",
    "\n",
    "# and defining the predict method which will predict Y from the X test data\n",
    "    def predict(self,X_test):\n",
    "        # needs to return a list of predictions\n",
    "        predictions=[]\n",
    "        for row in X_test:\n",
    "            label=random.choice(self.y_train)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=.5)\n",
    "\n",
    "# now we'll change our pipeline to use our new class instead of the imported KNeighborsClassifier from before\n",
    "my_classifier=ScrappyKNN()\n",
    "my_classifier.fit(X_train, y_train)\n",
    "predictions=my_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(y_test, predictions))\n",
    "\n",
    "# the answer here will be close to 0.33 - since we have three types of iris' and populated the label randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# now we're going to build out a K nearest neighbor classifier\n",
    "# all this does is find a certain # of neighbors near a point\n",
    "# whatever the majority of them are, that's what we predict the point to be\n",
    "# this uses something like the euclidean pythagoreum theorum (a^2+b^2=c^2) to find the distance\n",
    "# we can use a function that anaconda has available to us to help measure the distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# here a may become a point from our training data, and b a point from testing data\n",
    "def euc(a,b):\n",
    "    return distance.euclidean(a,b)\n",
    "\n",
    "class ScrappyKNN():\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train=X_train\n",
    "        self.y_train=y_train\n",
    "        pass\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        # needs to return a list of predictions, so make this an array\n",
    "        predictions=[]\n",
    "        for row in X_test:\n",
    "            # this time we replace random label with label of closest point to the test point\n",
    "            # will create this \"closest\" function in a second\n",
    "            label=self.closest(row)\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    def closest(self,row):\n",
    "        # this variable keep track of the closest one we found so far\n",
    "        # remember \"row\" is each row in x test - so we're measuring distance of that to points in training data\n",
    "        # this is measuring distance in multi-dimensional space using all the dimensions of the two rows\n",
    "        # initiate by comparing the test to first row (row 0) of the training data and setting that as best distance\n",
    "        best_dist=euc(row,self.X_train[0])\n",
    "        best_index=0\n",
    "        # iterate over all the other points, and everytime we find a closer one, update best distance and row index\n",
    "        for i in range(1,len(self.X_train)):\n",
    "            dist=euc(row,self.X_train[i])\n",
    "            if dist<best_dist:\n",
    "                best_dist=dist\n",
    "                best_index=i\n",
    "        return self.y_train[best_index]\n",
    "    \n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=.5)\n",
    "\n",
    "my_classifier=ScrappyKNN()\n",
    "my_classifier.fit(X_train, y_train)\n",
    "predictions=my_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# episode 6 - building our own image classifier using TensorFlow for Poets\n",
    "# This classifier uses deep learning to classify images with labels\n",
    "# Here x is a 2D image (e.g., png or jpg file), and y is a label like \"rose\"\n",
    "# Why is deep learning so great? Because it will extract the features itself from the image pixels\n",
    "# We don't need to create our own features such as \"bumpy texture\", etc., etc.\n",
    "# TF Learn is an easy to use library that sits on top of TensorFlow - similar to sklearn that we used earlier\n",
    "# In this case, TF for Poets leverages Inception - a huge image classification model with millions of parameters...\n",
    "# ...That can differentiate a large number of impage types\n",
    "# When you add your data it only trains the final layer of that network, so training takes \"reasonable\" time\n",
    "# According to Josh, the script will take about 20 minutes to train the classifier\n",
    "# GO TO TENSORFLOW FOR POETS WEBSITE AND FOLLOW INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
